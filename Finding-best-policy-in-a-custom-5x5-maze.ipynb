{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy grid (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 3 2]\n",
      " [0 0 0 0 2]\n",
      " [0 0 0 1 2]\n",
      " [0 0 1 1 2]\n",
      " [0 1 1 1 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The model of the maze environment is provided for your reference.\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "ACTION_UP    =  0\n",
    "ACTION_RIGHT =  1\n",
    "ACTION_DOWN  =  2\n",
    "ACTION_LEFT  =  3\n",
    "\n",
    "\n",
    "\n",
    "class GridWorld(object):\n",
    "    def __init__(self,shape=[5,5]):\n",
    "        self.shape = shape\n",
    "\n",
    "\n",
    "        numStates  = shape[0] * shape[1]\n",
    "        numActions = 4\n",
    "        self.numStates = numStates\n",
    "        self.numActions = numActions\n",
    "        \n",
    "        xmax = shape[0]\n",
    "        ymax = shape[1]\n",
    "\n",
    "        grid = np.arange(numStates).reshape(shape)\n",
    "\n",
    "        Model = {}\n",
    "\n",
    "        x_indices = np.arange(xmax)\n",
    "        y_indices = np.arange(ymax)\n",
    "\n",
    "        for x in x_indices:\n",
    "            for y in y_indices:\n",
    "                state = y + x*(xmax)\n",
    "                #print(x,y,state)\n",
    "                Model[state] ={action:[] for action in np.arange(numActions)}\n",
    "\n",
    "                is_terminal_state = lambda state : state == 0 or state == (numStates-1)\n",
    "                reward = 0.0 if is_terminal_state(state) else -1.0\n",
    "\n",
    "\n",
    "                if is_terminal_state(state):\n",
    "                    Model[state][ACTION_UP] = [(1.0,state,reward,True)]\n",
    "                    Model[state][ACTION_RIGHT] = [(1.0,state,reward,True)]\n",
    "                    Model[state][ACTION_DOWN] = [(1.0,state,reward,True)]\n",
    "                    Model[state][ACTION_LEFT] = [(1.0,state,reward,True)]\n",
    "                else:\n",
    "                    next_state = {}\n",
    "                    next_state[ACTION_UP] = state if x == 0 else state - ymax\n",
    "                    next_state[ACTION_RIGHT] = state if y == ymax-1 else state +1\n",
    "                    next_state[ACTION_DOWN] = state if x == xmax-1 else state + ymax\n",
    "                    next_state[ACTION_LEFT] = state if y == 0 else state -1 \n",
    "                    Model[state][ACTION_UP] = [(1.0,next_state[ACTION_UP] ,reward,is_terminal_state(next_state[ACTION_UP]))]\n",
    "                    Model[state][ACTION_RIGHT] = [(1.0,next_state[ACTION_RIGHT],reward,is_terminal_state(next_state[ACTION_RIGHT]))]\n",
    "                    Model[state][ACTION_DOWN] = [(1.0,next_state[ACTION_DOWN],reward,is_terminal_state(next_state[ACTION_DOWN]))]\n",
    "                    Model[state][ACTION_LEFT] = [(1.0,next_state[ACTION_LEFT],reward,is_terminal_state(next_state[ACTION_LEFT]))]\n",
    "        self.model = Model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "\n",
    "# Helper function to calculate the value for all actions in a given state    \n",
    "    def compute_value_fn_update(state,value_fn):\n",
    "        value_fn_update = np.zeros(env.numActions)\n",
    "        for action in range(env.numActions):\n",
    "            for prob,next_state,reward,done in env.model[state][action]:\n",
    "                value_fn_update[action] += prob * (reward + discount_factor * value_fn[next_state])\n",
    "                \n",
    "        return value_fn_update \n",
    "    \n",
    "    value_fn = np.zeros(env.numStates)\n",
    "    while True:\n",
    "# Stopping Condition        \n",
    "        delta = 0\n",
    "# Update each state        \n",
    "        for state in range(env.numStates):\n",
    "# Find the best action\n",
    "            action_values = compute_value_fn_update(state, value_fn)\n",
    "            best_action_value = np.max(action_values)\n",
    "# Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - value_fn[state]))\n",
    "# Update the value function\n",
    "            value_fn[state] = best_action_value        \n",
    "# Check if we can stop       \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy by using the optimal value function\n",
    "    policy = np.zeros([env.numStates, env.numActions])\n",
    "    for state in range(env.numStates):\n",
    "    # Find the best action for this state\n",
    "        A = compute_value_fn_update(state, value_fn)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[state, best_action] = 1.0\n",
    "    \n",
    "    return policy, value_fn\n",
    "\n",
    "\n",
    "\n",
    "# Write the code to create a deterministic policy by using the optimal value function. Print the optimal policy.\n",
    "\n",
    "     # Write your code here\n",
    "    \n",
    "env = GridWorld()\n",
    "policy, value_fn = value_iteration(env)\n",
    "\n",
    "\n",
    "\n",
    "# Save the final output as described in the sample format given below \n",
    "\n",
    "print(\"Policy grid (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "data=(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "output=pd.DataFrame(data)\n",
    "output.to_csv('output.csv', header=False, index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
